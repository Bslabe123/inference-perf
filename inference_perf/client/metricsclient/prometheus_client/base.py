# Copyright 2025 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from abc import abstractmethod
import logging
import time
from typing import List, Any
import requests
from inference_perf.config import PrometheusClientConfig
from ..base import MetricsClient, MetricsMetadata, PerfRuntimeParameters

PROMETHEUS_SCRAPE_BUFFER_SEC = 2

logger = logging.getLogger(__name__)


# When evaluated, returns a summary of the metric as a map, summary contents depends on the metric type
class PrometheusMetric:
    def __init__(self, name: str, filters: List[str]) -> None:
        self.name = name
        self.filters = ",".join(filters)

    @abstractmethod
    def get_queries(self, duration: float) -> dict[str, str]:
        raise NotImplementedError


# When evaluated, returns a single value
class PrometheusSingleMetric:
    def __init__(self, op: str, metric: PrometheusMetric) -> None:
        self.op = op
        self.metric = metric

    def get_query(self, duration: float) -> str:
        query = self.metric.get_queries(duration)
        if self.op in query:
            return query[self.op]
        raise Exception(f"query of type {'test'}, does not contain the operation {self.op}")


class PrometheusGaugeMetric(PrometheusMetric):
    def __init__(self, name: str, filters: List[str]) -> None:
        super().__init__(name, filters)

    def get_queries(self, duration: float) -> dict[str, str]:
        return {
            "mean": "avg_over_time(%s{%s}[%.0fs])" % (self.name, self.filters, duration),
            "median": "quantile_over_time(0.5, %s{%s}[%.0fs])" % (self.name, self.filters, duration),
            "sd": "stddev_over_time(%s{%s}[%.0fs])" % (self.name, self.filters, duration),
            "min": "min_over_time(%s{%s}[%.0fs])" % (self.name, self.filters, duration),
            "max": "max_over_time(%s{%s}[%.0fs])" % (self.name, self.filters, duration),
            "p90": "quantile_over_time(0.9, %s{%s}[%.0fs])" % (self.name, self.filters, duration),
            "p99": "quantile_over_time(0.99, %s{%s}[%.0fs])" % (self.name, self.filters, duration),
        }


class PrometheusCounterMetric(PrometheusMetric):
    def __init__(self, name: str, filters: List[str]) -> None:
        super().__init__(name, filters)

    def get_queries(self, duration: float) -> dict[str, str]:
        """
        Generates PromQL queries for a counter metric.

        Args:
            duration: The total time window to query over, in seconds (e.g., 3600 for 1 hour).
        """
        resolution = 15.0

        # Standard queries that don't need subqueries
        queries = {
            "rate": f"sum(rate({self.name}{{{self.filters}}}[{duration:.0f}s]))",
            "increase": f"sum(increase({self.name}{{{self.filters}}}[{duration:.0f}s]))",
        }

        # Subquery to first calculate the rate over many small intervals (the resolution)
        # across the total duration.
        subquery = f"rate({self.name}{{{self.filters}}}[{resolution:.0f}s])[{duration:.0f}s:{resolution:.0f}s]"

        # Aggregations over the series of rates generated by the subquery
        queries.update(
            {
                "mean_rate": f"avg_over_time({subquery})",
                "max_rate": f"max_over_time({subquery})",
                "min_rate": f"min_over_time({subquery})",
                "p90_rate": f"quantile_over_time(0.9, {subquery})",
                "p99_rate": f"quantile_over_time(0.99, {subquery})",
            }
        )
        return queries


class PrometheusHistogramMetric(PrometheusMetric):
    def __init__(self, name: str, filters: List[str]) -> None:
        super().__init__(name, filters)

    def get_queries(self, duration: float) -> dict[str, str]:
        return {
            "mean": "sum(rate(%s_sum{%s}[%.0fs])) / (sum(rate(%s_count{%s}[%.0fs])) > 0)"
            % (self.name, self.filters, duration, self.name, self.filters, duration),
            "median": "histogram_quantile(0.5, sum(rate(%s_bucket{%s}[%.0fs])) by (le))" % (self.name, self.filters, duration),
            "min": "histogram_quantile(0, sum(rate(%s_bucket{%s}[%.0fs])) by (le))" % (self.name, self.filters, duration),
            "max": "histogram_quantile(1, sum(rate(%s_bucket{%s}[%.0fs])) by (le))" % (self.name, self.filters, duration),
            "p90": "histogram_quantile(0.9, sum(rate(%s_bucket{%s}[%.0fs])) by (le))" % (self.name, self.filters, duration),
            "p99": "histogram_quantile(0.99, sum(rate(%s_bucket{%s}[%.0fs])) by (le))" % (self.name, self.filters, duration),
        }


class PrometheusMetricsClient(MetricsClient):
    def __init__(self, config: PrometheusClientConfig) -> None:
        if config:
            if not config.url:
                raise Exception("prometheus url missing")
            self.query_url = config.url.unicode_string().rstrip("/") + "/api/v1/query"
            logger.debug(f"Prometheus metrics client configured, querying metrics from '{self.query_url}'")
            self.scrape_interval = config.scrape_interval or 30
        else:
            raise Exception("prometheus config missing")

    def wait(self) -> None:
        """
        Waits for the Prometheus server to scrape the metrics.
        We have added a buffer of 5 seconds to the scrape interval to ensure that metrics for even the last request are collected.
        """
        wait_time = self.scrape_interval + PROMETHEUS_SCRAPE_BUFFER_SEC
        time.sleep(wait_time)

    def collect_metrics_summary(self, runtime_parameters: PerfRuntimeParameters) -> dict[str, Any] | None:
        """
        Collects the summary metrics for the given Perf Benchmark run.

        Args:
        runtime_parameters: The runtime parameters containing details about the Perf Benchmark like the duration and model server client

        Returns:
        A map of model server metric names to their results across all stages.
        """
        if runtime_parameters is None:
            logger.warning("Perf Runtime parameters are not set, skipping metrics collection")
            return None

        # Get the duration and model server client from the runtime parameters
        query_eval_time = time.time()
        query_duration = query_eval_time - runtime_parameters.start_time

        return self.get_model_server_metrics(runtime_parameters.model_server_metrics, query_duration, query_eval_time)

    def collect_metrics_for_stage(self, runtime_parameters: PerfRuntimeParameters, stage_id: int) -> dict[str, Any] | None:
        """
        Collects the summary metrics for a specific stage.

        Args:
        runtime_parameters: The runtime parameters containing details about the Perf Benchmark like the duration and model server client
        stage_id: The ID of the stage for which to collect metrics

        Returns:
        A map of model server metric names to their results for the specified stage.
        """
        if runtime_parameters is None:
            logger.warning("Perf Runtime parameters are not set, skipping metrics collection")
            return None

        if runtime_parameters.stages is None or stage_id not in runtime_parameters.stages:
            logger.warning(
                f"Stage ID {stage_id} is not present in the runtime parameters, skipping metrics collection for this stage"
            )
            return None

        # Get the query evaluation time and duration for the stage
        # The query evaluation time is the end time of the stage plus the scrape interval and a buffer to ensure metrics are collected
        # Duration is calculated as the difference between the eval time and start time of the stage
        logger.debug(f"runtime parameters for stage {stage_id}: {runtime_parameters}")
        query_eval_time = runtime_parameters.stages[stage_id].end_time + self.scrape_interval + PROMETHEUS_SCRAPE_BUFFER_SEC
        query_duration = query_eval_time - runtime_parameters.stages[stage_id].start_time
        return self.get_model_server_metrics(runtime_parameters.model_server_metrics, query_duration, query_eval_time)

    def get_model_server_metrics(
        self, metrics_metadata: MetricsMetadata, query_duration: float, query_eval_time: float
    ) -> dict[str, Any] | None:
        """
        Collects the summary metrics for the given Model Server Client and query duration.

        Args:
        metrics_metadata: TODO
        query_duration: The duration for which to collect metrics
        query_eval_time: The time at which the query is evaluated, used to ensure we are querying the correct time range

        Returns:
        A map of model server metrics to their results.
        """

        metric_to_result: dict[str, Any] = {}
        if not metrics_metadata:
            logger.warning("Metrics metadata is not present for the runtime")
            return None
        for metric_name, metric_metadata_item in metrics_metadata.items():
            if isinstance(metric_metadata_item, PrometheusMetric):
                metric = {}
                for key, query in metric_metadata_item.get_queries(query_duration).items():
                    result = self.execute_query(query, str(query_eval_time))
                    if result is None:
                        logger.error("Error executing query: %s", query)
                        continue
                    metric[key] = result
                metric_to_result[metric_name] = metric
            elif isinstance(metric_metadata_item, PrometheusSingleMetric):
                query = metric_metadata_item.get_query(query_duration)
                result = self.execute_query(query, str(query_eval_time))
                if result is not None:
                    metric_to_result[metric_name] = result
                else:
                    logger.error("Error executing query: %s", query)
        return metric_to_result

    def execute_query(self, query: str, eval_time: str) -> float:
        """
        Executes the given query on the Prometheus server and returns the result.

        Args:
        query: the PromQL query to execute
        eval_time: the time at which the query is evaluated, used to ensure we are querying the correct time range

        Returns:
        The result of the query.
        """
        query_result = 0.0
        try:
            logger.debug(f"making PromQL query: '{query}'")
            response = requests.get(self.query_url, headers=self.get_headers(), params={"query": query, "time": eval_time})
            if response is None:
                logger.error("error executing query: %s" % (query))
                return query_result

            response.raise_for_status()
        except Exception as e:
            logger.error("error executing query: %s" % (e))
            return query_result

        # Check if the response is valid
        # Sample response:
        # {
        #     "status": "success",
        #     "data": {
        #         "resultType": "vector",
        #         "result": [
        #             {
        #                 "metric": {},
        #                 "value": [
        #                     1632741820.781,
        #                     "0.0000000000000000"
        #                 ]
        #             }
        #         ]
        #     }
        # }

        response_obj = response.json()
        logger.debug(f"got result for query '{query}': {response_obj}")
        if response_obj.get("status") != "success":
            logger.error("error executing query: %s" % (response_obj))
            return query_result

        data = response_obj.get("data", {})
        result = data.get("result", [])
        if len(result) > 0 and "value" in result[0]:
            if isinstance(result[0]["value"], list) and len(result[0]["value"]) > 1:
                # Return the value of the first result
                # The value is in the second element of the list
                # e.g. [1632741820.781, "0.0000000000000000"]
                # We need to convert it to float
                # and return it
                # Convert the value to float
                try:
                    query_result = round(float(result[0]["value"][1]), 6)
                except ValueError:
                    logger.error("error converting value to float: %s" % (result[0]["value"][1]))
                    return query_result
        logger.debug(f"inferred result from query '{query}': {query_result}")
        return query_result

    def get_headers(self) -> dict[str, Any]:
        return {}
